{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a NB Classifier from scrach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Make sure this first cell let's chdir point to the project's folder with name  \"NaiveBayesClassifier\"\n",
    "#Make imports work \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\OneDrive\\Desktop\\CPSC 481-07 (AI)\\Code Examples\\NaiveBayesClassifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd()) #show current dir (should end with NaiveBayesClassier)\n",
    "\n",
    "#if not:\n",
    "if os.getcwd().split('\\\\')[-1] != 'NaiveBayesClassifier' :\n",
    "    os.chdir('c:\\\\Users\\\\Benjamin\\\\OneDrive\\\\Desktop\\\\CPSC 481-07 (AI)\\\\Code Examples\\\\NaiveBayesClassifier\\\\') #maybe adjust this path to point at right dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "this CountVectorizer is quite useful. It counts the words and represents\n",
    "\n",
    "- each sentence / email as a row\n",
    "- each word / term as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'blue', 'green', 'house', 'window']\n",
      "[[1 2 0 1 1]\n",
      " [0 0 1 1 2]\n",
      " [1 2 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"blue house and blue window\", \"window window green house\", \"green house and blue blue\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_count_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# The unique words / column names\n",
    "print(vectorizer.get_feature_names()) #in our sklearn version it is get_feature_names, in newer versions it's get_feature_names_out\n",
    "\n",
    "# Each word\"s amount of occurences in texts\n",
    "print(term_count_matrix.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at our data in data_cooked.csv which looks something like this (example)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "urgent contact last weekend draw show prize call claim code valid,1\n",
    "\n",
    "get dump heap mom decid come low bore,0\n",
    "\n",
    "ok lor soni ericsson salesman ask shuhui say quit gd use consid,0\n",
    "\n",
    "privat account statement show un redeem point call identifi code expir,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bore', 'call', 'claim', 'code', 'free', 'heap', 'identifi', 'mom', 'ok', 'quit', 'salesman', 'urgent']\n",
      "Word Count Matrix looks like this:\n",
      " [[0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 1 0 1 1 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#First get the data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#I took this from our prepared data and adjusted a bit to illustrate better\n",
    "data= np.array([\n",
    "(\"free claim call code\", 1),\n",
    "(\"heap mom bore\",0),\n",
    "(\"ok salesman quit\",0),\n",
    "(\"urgent call free identifi code\",1)\n",
    "])\n",
    "\n",
    "mails = data[:,0] #all rows, 0th column\n",
    "labels = data[:,1] #all rows, 1st column\n",
    "\n",
    "\n",
    "# Second: use the vectorizer on the mails\n",
    "vectorizer = CountVectorizer()\n",
    "word_count_matrix = vectorizer.fit_transform(mails)\n",
    "print(vectorizer.get_feature_names()) # Column Names \n",
    "print('Word Count Matrix looks like this:\\n', word_count_matrix.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a next step, we can calculating:\n",
    "\n",
    "for each word (meaning for each column): Count how often the word appears in a spam sentence and then calculate its probability as \n",
    "\n",
    "Occurences in Spam Mails  /  Amount all mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bore': 0.25,\n",
       " 'call': 0.5,\n",
       " 'claim': 0.25,\n",
       " 'code': 0.5,\n",
       " 'free': 0.5,\n",
       " 'heap': 0.25,\n",
       " 'identifi': 0.25,\n",
       " 'mom': 0.25,\n",
       " 'ok': 0.25,\n",
       " 'quit': 0.25,\n",
       " 'salesman': 0.25,\n",
       " 'urgent': 0.25}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = {}\n",
    "\n",
    "\n",
    "number_all_mails = len(word_count_matrix.toarray())\n",
    "for index,word in enumerate(vectorizer.get_feature_names()):\n",
    "    column = word_count_matrix[:,index]\n",
    "    count = np.count_nonzero(column[column>0]) # column[column>0] makes column a 1D array of True and False values \n",
    "    probabilities[word] = count / number_all_mails\n",
    "\n",
    "probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how words like call, code, free are rated higher than other words\n",
    "\n",
    "it's still very simple the way it is but could be a start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NBC from scratch\n",
    "\n",
    "In the following code cells I tried to code the Naive Bayes Calssifier from our lecture, starting at slide number 28 in \"Reasoning with uncertainty.pptx\".\n",
    "\n",
    "This preparing the data and 2 attempts of implementing the classifier.\n",
    "\n",
    "**Each of them is commented out** since later on in the project, we wrapped the code into method definitions. This way, not every notebook run is slowed by running the next 3 code blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_path = \"Data/Preprocessing/data_cooked.csv\"\\n\\ndataframe = pd.read_csv(data_path)\\n\\n# somehow there are missing values somewhere, may have to check Preprocessing.ipynb again\\n# very important to reset index! Because pandas would otherwise memorize the old indices which is bad for what is about to come\\ndataframe = dataframe.dropna(subset=[\\'Message\\']).reset_index(drop=True)\\n\\nmails = dataframe[\\'Message\\']\\nlabels = dataframe[\\'Category\\']\\n\\n#cv = CountVectorizer()\\n#word_count_matrix = cv.fit_transform(mails.to_numpy()).toarray()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start by getting some data and making it usable:\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "data_path = \"Data/Preprocessing/data_cooked.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "# somehow there are missing values somewhere, may have to check Preprocessing.ipynb again\n",
    "# very important to reset index! Because pandas would otherwise memorize the old indices which is bad for what is about to come\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "mails = dataframe['Message']\n",
    "labels = dataframe['Category']\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "#word_count_matrix = cv.fit_transform(mails.to_numpy()).toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n#Calculating P(SPAM) and P(not SPAM)\\nspam_count_total = len(labels.loc[labels==1])\\nnot_spam_count_total = len(labels.loc[labels==0])\\ntotal = spam_count_total + not_spam_count_total\\n\\nP_spam = spam_count_total / total\\nP_not_spam = not_spam_count_total / total\\n\\n#Calculating the Conditional Probability for each word:\\n\\n\\n# An array that counts the values for each word\\nunique_word_count = len(word_count_matrix[0]) #how many elements does one row have + eunique words in corpus\\n\\nwords = cv.get_feature_names()\\nword_proba_array = np.zeros((2,unique_word_count)) \\n#2 rows:\\n# FIRST ROW for the word\\'s cond. probab for spam=yes\\n# SECOND ROW for the word\\'s probab for spam=no\\n#(see slide 32 in lecture)\\n\\n#Y columns: as many colums as unique words there are in data\\n\\n\\n#Laplace thing to not get 0-probabs\\nalpha = 1\\n\\n#Get the idices for all spam rows and for all non spam rows\\nspam_rows_indices = labels[labels == 1].index.tolist()\\nnot_spam_rows_indices = labels[labels == 0].index.tolist()\\n\\n# Calculate the two conditional probabilities for each word\\nfor word_idx in range(len(word_count_matrix[0])):\\n    \\n    #calculating for spam=yes rows\\n    for row_index in spam_rows_indices:\\n        word_count = word_count_matrix[row_index][word_idx]\\n        if(word_count > 0): #only checks if the word appears, not how often\\n            word_proba_array[0][word_idx] += 1 / (spam_count_total)\\n            #-> word_proba_array[1] means writing to the row which is for concitional probabs for Spam=Yes\\n            #-> word_proba_array[1][word_idx] means we\\'re writing to the index of the word we\"re looking at right now\\n            #-> \"+= 1 / spam_count_total\" increases the probability by one each time a word is found for a spam=yes mail\\n\\n\\n    #calculating for spam=no rows\\n    for row_index in not_spam_rows_indices:\\n        word_count = word_count_matrix[row_index][word_idx]\\n        if(word_count > 0): #only checks if the word appears, not how often\\n            word_proba_array[1][word_idx] += 1 / (not_spam_count_total)\\n            #-> same as before but for spam=no mails\\n\\n\\n\\n#COncatenating words with their probabilities\\n\\nprobabilities_df = pd.DataFrame({\\n    \\'Word\\': words,\\n    \\'P(Word|Spam)\\': word_proba_array[0],\\n    \\'P(Word|NotSpam)\\': word_proba_array[1]\\n})\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.Attempt\n",
    "#CAUTION: This was my initial attempt which follows exactly what we did in the lecture on slide 32\n",
    "#but it takes a long time to execute. See a below cell that uses numpy matrix operations\n",
    "\n",
    "#(Commented this code because it's wrapped in a method later on andI don\"t want\n",
    "#this cell to run in each notebook run and take up extra time)\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "#Calculating P(SPAM) and P(not SPAM)\n",
    "spam_count_total = len(labels.loc[labels==1])\n",
    "not_spam_count_total = len(labels.loc[labels==0])\n",
    "total = spam_count_total + not_spam_count_total\n",
    "\n",
    "P_spam = spam_count_total / total\n",
    "P_not_spam = not_spam_count_total / total\n",
    "\n",
    "#Calculating the Conditional Probability for each word:\n",
    "\n",
    "\n",
    "# An array that counts the values for each word\n",
    "unique_word_count = len(word_count_matrix[0]) #how many elements does one row have + eunique words in corpus\n",
    "\n",
    "words = cv.get_feature_names()\n",
    "word_proba_array = np.zeros((2,unique_word_count)) \n",
    "#2 rows:\n",
    "# FIRST ROW for the word's cond. probab for spam=yes\n",
    "# SECOND ROW for the word's probab for spam=no\n",
    "#(see slide 32 in lecture)\n",
    "\n",
    "#Y columns: as many colums as unique words there are in data\n",
    "\n",
    "\n",
    "#Laplace thing to not get 0-probabs\n",
    "alpha = 1\n",
    "\n",
    "#Get the idices for all spam rows and for all non spam rows\n",
    "spam_rows_indices = labels[labels == 1].index.tolist()\n",
    "not_spam_rows_indices = labels[labels == 0].index.tolist()\n",
    "\n",
    "# Calculate the two conditional probabilities for each word\n",
    "for word_idx in range(len(word_count_matrix[0])):\n",
    "    \n",
    "    #calculating for spam=yes rows\n",
    "    for row_index in spam_rows_indices:\n",
    "        word_count = word_count_matrix[row_index][word_idx]\n",
    "        if(word_count > 0): #only checks if the word appears, not how often\n",
    "            word_proba_array[0][word_idx] += 1 / (spam_count_total)\n",
    "            #-> word_proba_array[1] means writing to the row which is for concitional probabs for Spam=Yes\n",
    "            #-> word_proba_array[1][word_idx] means we're writing to the index of the word we\"re looking at right now\n",
    "            #-> \"+= 1 / spam_count_total\" increases the probability by one each time a word is found for a spam=yes mail\n",
    "\n",
    "\n",
    "    #calculating for spam=no rows\n",
    "    for row_index in not_spam_rows_indices:\n",
    "        word_count = word_count_matrix[row_index][word_idx]\n",
    "        if(word_count > 0): #only checks if the word appears, not how often\n",
    "            word_proba_array[1][word_idx] += 1 / (not_spam_count_total)\n",
    "            #-> same as before but for spam=no mails\n",
    "\n",
    "\n",
    "\n",
    "#COncatenating words with their probabilities\n",
    "\n",
    "probabilities_df = pd.DataFrame({\n",
    "    'Word': words,\n",
    "    'P(Word|Spam)': word_proba_array[0],\n",
    "    'P(Word|NotSpam)': word_proba_array[1]\n",
    "})\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this next cell once crashed with a MemoryError for me. But after I rebooted my laptop, it had no more problems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nspam_count_total = np.sum(labels == 1)\\nnot_spam_count_total = np.sum(labels == 0)\\ntotal = spam_count_total + not_spam_count_total\\n\\nP_spam = spam_count_total / total\\nP_not_spam = not_spam_count_total / total\\n\\nalpha = 1\\nunique_word_count = len(cv.get_feature_names())\\nwords = np.array(cv.get_feature_names())\\n\\n#instead of looping through the whole dataset counting word appearcances and adding probabilities, make \\n#an array that counts each word's appearances over the whole dataset (also considers how often a word\\n# appears in a sentence and not only if or if it doean's appear as in the 1.attempt)\\nspam_word_counts = word_count_matrix[labels == 1].sum(axis=0)\\nnot_spam_word_counts = word_count_matrix[labels == 0].sum(axis=0)\\n\\n#Calculating the contiditional probalbilities by going  spam_word_counts/ total amount of spam word\\n# + it adds laplace smoothing as seen here:  https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\\nword_proba_spam = (spam_word_counts + alpha) / (spam_word_counts.sum() + alpha * unique_word_count)\\nword_proba_not_spam = (not_spam_word_counts + alpha) / (not_spam_word_counts.sum() + alpha * unique_word_count)\\n\\n\\n\\nprobabilities_df = pd.DataFrame({\\n    'Word': words,\\n    'P(Word|Spam)': word_proba_spam,\\n    'P(Word|NotSpam)': word_proba_not_spam\\n})\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd Attempt (Commented this code because it's wrapped in a method later on andI don\"t want\n",
    "#this cell to run in each notebook run and take up extra time)\n",
    "\"\"\"\n",
    "spam_count_total = np.sum(labels == 1)\n",
    "not_spam_count_total = np.sum(labels == 0)\n",
    "total = spam_count_total + not_spam_count_total\n",
    "\n",
    "P_spam = spam_count_total / total\n",
    "P_not_spam = not_spam_count_total / total\n",
    "\n",
    "alpha = 1\n",
    "unique_word_count = len(cv.get_feature_names())\n",
    "words = np.array(cv.get_feature_names())\n",
    "\n",
    "#instead of looping through the whole dataset counting word appearcances and adding probabilities, make \n",
    "#an array that counts each word's appearances over the whole dataset (also considers how often a word\n",
    "# appears in a sentence and not only if or if it doean's appear as in the 1.attempt)\n",
    "spam_word_counts = word_count_matrix[labels == 1].sum(axis=0)\n",
    "not_spam_word_counts = word_count_matrix[labels == 0].sum(axis=0)\n",
    "\n",
    "#Calculating the contiditional probalbilities by going  spam_word_counts/ total amount of spam word\n",
    "# + it adds laplace smoothing as seen here:  https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "word_proba_spam = (spam_word_counts + alpha) / (spam_word_counts.sum() + alpha * unique_word_count)\n",
    "word_proba_not_spam = (not_spam_word_counts + alpha) / (not_spam_word_counts.sum() + alpha * unique_word_count)\n",
    "\n",
    "\n",
    "\n",
    "probabilities_df = pd.DataFrame({\n",
    "    'Word': words,\n",
    "    'P(Word|Spam)': word_proba_spam,\n",
    "    'P(Word|NotSpam)': word_proba_not_spam\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write this as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probas(mails:pd.DataFrame, labels:pd.DataFrame):\n",
    "\n",
    "    spam_count_total = np.sum(labels == 1)\n",
    "    not_spam_count_total = np.sum(labels == 0)\n",
    "    total = spam_count_total + not_spam_count_total\n",
    "\n",
    "    P_spam = spam_count_total / total\n",
    "    P_not_spam = not_spam_count_total / total\n",
    "\n",
    "    ret_dict ={\n",
    "        \"P_spam\" : P_spam, \n",
    "        \"P_not_spam\": P_not_spam\n",
    "    } \n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell defines two methods: \n",
    "- **get_cond_probas_attempt_1()** that uses the code from the 1st attempt and \n",
    "- **get_cond_probas_attempt_2()** that uses the code from the 2nd attempt \n",
    "\n",
    "\n",
    "to calculate the conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_probas_attempt_1(p_mails:pd.DataFrame, p_labels:pd.DataFrame):\n",
    "    p_mails = p_mails.reset_index(drop=True)\n",
    "    p_labels = p_labels.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    word_count_matrix = cv.fit_transform(p_mails.to_numpy()).toarray()\n",
    "\n",
    "    spam_count_total = np.sum(p_labels == 1)\n",
    "    not_spam_count_total = np.sum(p_labels == 0)\n",
    "    \n",
    "    unique_word_count = len(word_count_matrix[0]) #how many elements does one row have + eunique words in corpus\n",
    "\n",
    "    words = cv.get_feature_names()\n",
    "    word_proba_array = np.zeros((2,unique_word_count))\n",
    "    \n",
    "    #Get the idices for all spam rows and for all non spam rows\n",
    "    spam_rows_indices = p_labels[p_labels == 1].index.tolist()\n",
    "    not_spam_rows_indices = p_labels[p_labels == 0].index.tolist()\n",
    "\n",
    "    # Calculate the two conditional probabilities for each word\n",
    "    for word_idx in range(len(word_count_matrix[0])):\n",
    "        \n",
    "        #calculating for spam=yes rows\n",
    "        for row_index in spam_rows_indices:\n",
    "            word_count = word_count_matrix[row_index][word_idx]\n",
    "            if(word_count > 0): #only checks if the word appears, not how often\n",
    "                word_proba_array[0][word_idx] += 1 / (spam_count_total)\n",
    "\n",
    "        #calculating for spam=no rows\n",
    "        for row_index in not_spam_rows_indices:\n",
    "            word_count = word_count_matrix[row_index][word_idx]\n",
    "            if(word_count > 0): #only checks if the word appears, not how often\n",
    "                word_proba_array[1][word_idx] += 1 / (not_spam_count_total)\n",
    "                #-> same as before but for spam=no mails\n",
    "\n",
    "    probabilities_df = pd.DataFrame({\n",
    "        'Word': words,\n",
    "        'P(Word|Spam)': word_proba_array[0],\n",
    "        'P(Word|NotSpam)': word_proba_array[1]\n",
    "    })\n",
    "\n",
    "    return probabilities_df\n",
    "\n",
    "\n",
    "def get_cond_probas_attempt_2(p_mails:pd.DataFrame, p_labels:pd.DataFrame):\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    word_count_matrix = cv.fit_transform(p_mails.to_numpy()).toarray()\n",
    "\n",
    "    alpha = 1\n",
    "    unique_word_count = len(cv.get_feature_names())\n",
    "    words = np.array(cv.get_feature_names())\n",
    "\n",
    "    #instead of looping through the whole dataset counting word appearcances and adding probabilities, make \n",
    "    #an array that counts each word's appearances over the whole dataset (also considers how often a word\n",
    "    # appears in a sentence and not only if or if it doean's appear as in the 1.attempt)\n",
    "    spam_word_counts = word_count_matrix[p_labels == 1].sum(axis=0)\n",
    "    not_spam_word_counts = word_count_matrix[p_labels == 0].sum(axis=0)\n",
    "\n",
    "    #Calculating the contiditional probalbilities by going  spam_word_counts/ total amount of spam word\n",
    "    # + it adds laplace smoothing as seen here:  https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "    word_proba_spam = (spam_word_counts + alpha) / (spam_word_counts.sum() + alpha * unique_word_count)\n",
    "    word_proba_not_spam = (not_spam_word_counts + alpha) / (not_spam_word_counts.sum() + alpha * unique_word_count)\n",
    "\n",
    "    probabilities_df = pd.DataFrame({\n",
    "        'Word': words,\n",
    "        'P(Word|Spam)': word_proba_spam,\n",
    "        'P(Word|NotSpam)': word_proba_not_spam\n",
    "    })\n",
    "\n",
    "    return probabilities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>P(Word|Spam)</th>\n",
       "      <th>P(Word|NotSpam)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.003380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaenerfax</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aadedeji</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aagraw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  P(Word|Spam)  P(Word|NotSpam)\n",
       "0          aa      0.000946         0.003380\n",
       "1         aaa      0.000000         0.000654\n",
       "2  aaaenerfax      0.000000         0.000109\n",
       "3    aadedeji      0.000000         0.000109\n",
       "4      aagraw      0.000000         0.000109"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GIve it a test\n",
    "\n",
    "data_path = \"Data/Preprocessing/data_cooked.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "# somehow there are missing values somewhere, may have to check Preprocessing.ipynb again\n",
    "# very important to reset index! Because pandas would otherwise memorize the old indices which is bad for what is about to come\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "mails = dataframe['Message']\n",
    "labels = dataframe['Category']\n",
    "\n",
    "cond_probas = get_cond_probas_attempt_1(p_mails=mails,p_labels=labels)\n",
    "\n",
    "cond_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word  P(Word|Spam)  P(Word|NotSpam)\n",
      "3417       call      0.217967         0.135194\n",
      "4867    confirm      0.017967         0.034344\n",
      "9150       free      0.195272         0.055495\n",
      "9216     friend      0.034988         0.024313\n",
      "15768     money      0.133806         0.015373\n",
      "22408    signup      0.004728         0.000109\n",
      "24994  tomorrow      0.009929         0.034235\n",
      "26319     verif      0.008983         0.001090\n",
      "27228       win      0.046809         0.005233\n"
     ]
    }
   ],
   "source": [
    "#Just out of curiosity some smaple conditional probabilities\n",
    "examples = ['free', 'call', 'friend', \"tomorrow\", 'signup', 'money', 'win', 'verif', 'confirm']\n",
    "\n",
    "selected_probabilities = cond_probas[cond_probas['Word'].isin(examples)]\n",
    "\n",
    "print(selected_probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on slide number 45: Classifying spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for classifying a single message\n",
    "\n",
    "def classify_message(message, cond_probas, prior_probas):\n",
    "    \n",
    "\n",
    "    #the next line make a countvectorizer that uses the same \n",
    "    #columns names as in the training (column names = unique words)\n",
    "    #this is done by adding the same vocabulary as in Word\n",
    "    cv = CountVectorizer(vocabulary=cond_probas['Word'].values)\n",
    "\n",
    "    #with this CountVectorizer you can make a vector from the message\n",
    "    #same as the word_count_matrix, but only for one mail text\n",
    "    message_vector = cv.transform([message]).toarray().flatten()\n",
    "\n",
    "    #for the calculation it's somehow better to use log of the actual probabs\n",
    "    #avoids some kind of problem that arieses if one calculates with very small numbers\n",
    "    log_prob_spam = np.log(prior_probas['P_spam'])\n",
    "    log_prob_not_spam = np.log(prior_probas['P_not_spam'])\n",
    "\n",
    "    #in this loop the calculation for the vector is done for spam=yes as well as for spam=no\n",
    "    for word, count in zip(cv.get_feature_names(), message_vector): \n",
    "        if count > 0:  \n",
    "            if word in cond_probas['Word'].values: #could be that test data contains words that haavent been in training data\n",
    "                word_data = cond_probas[cond_probas['Word'] == word] # get the word's conditional probabilities\n",
    "                log_prob_spam += count * np.log(word_data['P(Word|Spam)'].values[0]) \n",
    "                log_prob_not_spam += count * np.log(word_data['P(Word|NotSpam)'].values[0])\n",
    "\n",
    "    #make the choice by comparing the two values\n",
    "    if log_prob_spam > log_prob_not_spam:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning\n",
    "# This cell may take a few minutes: \n",
    "# it loads the data, trains the model, makes predictions and calculates a score of the predictions\n",
    "\n",
    "#using train_test_split to make a train and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "mails = dataframe['Message']\n",
    "labels = dataframe['Category']\n",
    "\n",
    "#making a train set which is used to calculate the probabilities and a test set to test afterwards with non seen cases\n",
    "mails_train, mails_test, labels_train, labels_test = train_test_split(\n",
    "    mails, labels, test_size=0.25, random_state=42,stratify=labels) #random_state shuffles all lines in the dataset, stratify balances the calsses spam and no spam because there's a lot more spam=no than spam=yes in our data\n",
    "\n",
    "#calling our functions from earlier\n",
    "prior_probas = get_prior_probas(mails_train, labels_train)\n",
    "\n",
    "#Decide if you use get_cond_probas_attempt_1 or get_cond_probas_attempt_2 HERE:\n",
    "conditional_probas = get_cond_probas_attempt_2(mails_train, labels_train)\n",
    "\n",
    "#creating the result array with a list comprehension for each test mail in mails_test\n",
    "results = [classify_message(msg, conditional_probas, prior_probas) for msg in mails_test]\n",
    "\n",
    "\n",
    "#calculate mean of an array that looks like this [True,False,False,True,True,True...]\n",
    "#using a list comprehension that zips together the model's predictions in results and the actual\n",
    "#labels in labels_test\n",
    "#then with pred == real it creates a True/False value, indicating, if the prediction was right/wrong\n",
    "#finally using np.mean to create the average\n",
    "accuracy = np.mean([pred == real for pred, real in zip(results, labels_test)])\n",
    "print(f\"Accuracy is: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another NBC example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "data_path = \"Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "# There were some null values in the Message column so Idropped these \n",
    "dataframe = dataframe.dropna(subset=['Message']) \n",
    "\n",
    "dataframe.groupby('Category').describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataframe.Message, dataframe.Category)\n",
    "\n",
    "\n",
    "# get word count and store the data as a matrix \n",
    "cv = CountVectorizer()\n",
    "x_train_count = cv.fit_transform(x_train.values)\n",
    "\n",
    "x_train_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "naiveBayes = MultinomialNB()\n",
    "naiveBayes.fit(x_train_count, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-test for ham\n",
    "email_ham = [\"hey wanna meet up for dinner tonight?\"]\n",
    "email_ham_count = cv.transform(email_ham)\n",
    "naiveBayes.predict(email_ham_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-test for spam\n",
    "email_spam = [\"reward money insurance click win\"]\n",
    "email_spam_count = cv.transform(email_spam)\n",
    "naiveBayes.predict(email_spam_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model \n",
    "x_test_count = cv.transform(x_test)\n",
    "naiveBayes.score(x_test_count, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model with tensorflow ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: to use tensorflow, you'll have to\n",
    "\n",
    "1. conda remove --name spam_classifier_env --all (to remove the old environment)\n",
    "2. conda env create -f environment.yml (to create the new enviroment containing keras)\n",
    "3. conda activate spam_classifier_env (to activate the new environment)\n",
    "4. restart notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "#importing the packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and prepare\n",
    "data_path = \"Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = dataframe['Message']\n",
    "y = dataframe['Category']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(X).toarray()  # make it a matrix, the network can handle (not sparse matrix)\n",
    "\n",
    "\n",
    "#this will tell how the input shape of the network needs to look like\n",
    "print(len(cv.get_feature_names()))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, input_shape=[X_train.shape[1]], activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128,  activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128,  activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a while, reduce epochs do less training\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=3, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy on test data: {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
