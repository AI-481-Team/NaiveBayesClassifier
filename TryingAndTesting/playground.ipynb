{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a NB Classifier from scrach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "this CountVectorizer is quite useful. It counts the words and represents\n",
    "\n",
    "- each sentence / email as a row\n",
    "- each word / term as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'blue', 'green', 'house', 'window']\n",
      "[[1 2 0 1 1]\n",
      " [0 0 1 1 2]\n",
      " [1 2 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"blue house and blue window\", \"window window green house\", \"green house and blue blue\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_count_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# The unique words / column names\n",
    "print(vectorizer.get_feature_names()) #in our sklearn version it is get_feature_names, in newer versions it's get_feature_names_out\n",
    "\n",
    "# Each word\"s amount of occurences in texts\n",
    "print(term_count_matrix.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at our data in data_cooked.csv which looks something like this (example)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "urgent contact last weekend draw show prize call claim code valid,1\n",
    "\n",
    "get dump heap mom decid come low bore,0\n",
    "\n",
    "ok lor soni ericsson salesman ask shuhui say quit gd use consid,0\n",
    "\n",
    "privat account statement show un redeem point call identifi code expir,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bore', 'call', 'claim', 'code', 'free', 'heap', 'identifi', 'mom', 'ok', 'quit', 'salesman', 'urgent']\n",
      "Word Count Matrix looks like this:\n",
      " [[0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 1 0 1 1 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#First get the data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#I took this from our prepared data and adjusted a bit to illustrate better\n",
    "data= np.array([\n",
    "(\"free claim call code\", 1),\n",
    "(\"heap mom bore\",0),\n",
    "(\"ok salesman quit\",0),\n",
    "(\"urgent call free identifi code\",1)\n",
    "])\n",
    "\n",
    "mails = data[:,0] #all rows, 0th column\n",
    "labels = data[:,1] #all rows, 1st column\n",
    "\n",
    "\n",
    "# Second: use the vectorizer on the mails\n",
    "vectorizer = CountVectorizer()\n",
    "word_count_matrix = vectorizer.fit_transform(mails)\n",
    "print(vectorizer.get_feature_names()) # Column Names \n",
    "print('Word Count Matrix looks like this:\\n', word_count_matrix.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a next step, we can calculating:\n",
    "\n",
    "for each word (meaning for each column): Count how often the word appears in a spam sentence and then calculate its probability as \n",
    "\n",
    "Occurences in Spam Mails  /  Amount all mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bore': 0.25,\n",
       " 'call': 0.5,\n",
       " 'claim': 0.25,\n",
       " 'code': 0.5,\n",
       " 'free': 0.5,\n",
       " 'heap': 0.25,\n",
       " 'identifi': 0.25,\n",
       " 'mom': 0.25,\n",
       " 'ok': 0.25,\n",
       " 'quit': 0.25,\n",
       " 'salesman': 0.25,\n",
       " 'urgent': 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = {}\n",
    "\n",
    "\n",
    "number_all_mails = len(word_count_matrix.toarray())\n",
    "for index,word in enumerate(vectorizer.get_feature_names()):\n",
    "    column = word_count_matrix[:,index]\n",
    "    count = np.count_nonzero(column[column>0]) # column[column>0] makes column a 1D array of True and False values \n",
    "    probabilities[word] = count / number_all_mails\n",
    "\n",
    "probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how words like call, code, free are rated higher than other words\n",
    "\n",
    "it's still very simple the way it is but could be a start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cells I tried to code the Naive Bayes Calssifier from our lecture, starting at slide number 28 in \"Reasoning with uncertainty.pptx\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by getting some data and making it usable:\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../Data/Preprocessing/data_cooked.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "# somehow there are missing values somewhere, may have to check Preprocessing.ipynb again\n",
    "# very important to reset index! Because pandas would otherwise memorize the old indices which is bad for what is about to come\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "mails = dataframe['Message']\n",
    "labels = dataframe['Category']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "word_count_matrix = cv.fit_transform(mails.to_numpy()).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Attempt\n",
    "#CAUTION: This was my initial attempt which follows exactly what we did in the lecture on slide 32\n",
    "#but it takes a long time to execute. See a below cell that uses numpy matrix operations\n",
    "\n",
    "\n",
    "#Calculating P(SPAM) and P(not SPAM)\n",
    "spam_count_total = len(labels.loc[labels==1])\n",
    "not_spam_count_total = len(labels.loc[labels==0])\n",
    "total = spam_count_total + not_spam_count_total\n",
    "\n",
    "P_spam = spam_count_total / total\n",
    "P_not_spam = not_spam_count_total / total\n",
    "\n",
    "#Calculating the Conditional Probability for each word:\n",
    "\n",
    "\n",
    "# An array that counts the values for each word\n",
    "unique_word_count = len(word_count_matrix[0]) #how many elements does one row have + eunique words in corpus\n",
    "\n",
    "words = cv.get_feature_names()\n",
    "word_proba_array = np.zeros((2,unique_word_count)) \n",
    "#2 rows:\n",
    "# FIRST ROW for the word's cond. probab for spam=yes\n",
    "# SECOND ROW for the word's probab for spam=no\n",
    "#(see slide 32 in lecture)\n",
    "\n",
    "#Y columns: as many colums as unique words there are in data\n",
    "\n",
    "\n",
    "#Laplace thing to not get 0-probabs\n",
    "alpha = 1\n",
    "\n",
    "#Get the idices for all spam rows and for all non spam rows\n",
    "spam_rows_indices = labels[labels == 1].index.tolist()\n",
    "not_spam_rows_indices = labels[labels == 0].index.tolist()\n",
    "\n",
    "# Calculate the two conditional probabilities for each word\n",
    "for word_idx in range(len(word_count_matrix[0])):\n",
    "    \n",
    "    #calculating for spam=yes rows\n",
    "    for row_index in spam_rows_indices:\n",
    "        word_count = word_count_matrix[row_index][word_idx]\n",
    "        if(word_count > 0): #only checks if the word appears, not how often\n",
    "            word_proba_array[0][word_idx] = 1 / (spam_count_total)\n",
    "            #-> word_proba_array[1] means writing to the row which is for concitional probabs for Spam=Yes\n",
    "            #-> word_proba_array[1][word_idx] means we're writing to the index of the word we\"re looking at right now\n",
    "            #-> \"+= 1 / spam_count_total\" increases the probability by one each time a word is found for a spam=yes mail\n",
    "\n",
    "\n",
    "    #calculating for spam=no rows\n",
    "    for row_index in not_spam_rows_indices:\n",
    "        word_count = word_count_matrix[row_index][word_idx]\n",
    "        if(word_count > 0): #only checks if the word appears, not how often\n",
    "            word_proba_array[1][word_idx] += 1 / (not_spam_count_total)\n",
    "            #-> same as before but for spam=no mails\n",
    "\n",
    "\n",
    "\n",
    "#COncatenating words with their probabilities\n",
    "\n",
    "probabilities_df = pd.DataFrame({\n",
    "    'Word': words,\n",
    "    'P(Word|Spam)': word_proba_array[0],\n",
    "    'P(Word|NotSpam)': word_proba_array[1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Attempt \n",
    "\n",
    "spam_count_total = np.sum(labels == 1)\n",
    "not_spam_count_total = np.sum(labels == 0)\n",
    "total = spam_count_total + not_spam_count_total\n",
    "\n",
    "P_spam = spam_count_total / total\n",
    "P_not_spam = not_spam_count_total / total\n",
    "\n",
    "alpha = 1\n",
    "unique_word_count = len(cv.get_feature_names())\n",
    "words = np.array(cv.get_feature_names())\n",
    "\n",
    "#instead of looping through the whole dataset counting word appearcances and adding probabilities, make \n",
    "#an array that counts each word's appearances over the whole dataset (also considers how often a word\n",
    "# appears in a sentence and not only if or if it doean's appear as in the 1.attempt)\n",
    "spam_word_counts = word_count_matrix[labels == 1].sum(axis=0)\n",
    "not_spam_word_counts = word_count_matrix[labels == 0].sum(axis=0)\n",
    "\n",
    "#Calculating the contiditional probalbilities by going  spam_word_counts/ total amount of spam word\n",
    "# + it adds laplace smoothing as seen here:  https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "word_proba_spam = (spam_word_counts + alpha) / (spam_word_counts.sum() + alpha * unique_word_count)\n",
    "word_proba_not_spam = (not_spam_word_counts + alpha) / (not_spam_word_counts.sum() + alpha * unique_word_count)\n",
    "\n",
    "\n",
    "\n",
    "probabilities_df = pd.DataFrame({\n",
    "    'Word': words,\n",
    "    'P(Word|Spam)': word_proba_spam,\n",
    "    'P(Word|NotSpam)': word_proba_not_spam\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write this as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probas(mails:pd.DataFrame, labels:pd.DataFrame):\n",
    "\n",
    "    spam_count_total = np.sum(labels == 1)\n",
    "    not_spam_count_total = np.sum(labels == 0)\n",
    "    total = spam_count_total + not_spam_count_total\n",
    "\n",
    "    P_spam = spam_count_total / total\n",
    "    P_not_spam = not_spam_count_total / total\n",
    "\n",
    "    ret_dict ={\n",
    "        \"P_spam\" : P_spam, \n",
    "        \"P_not_spam\": P_not_spam\n",
    "    } \n",
    "    return ret_dict\n",
    "\n",
    "def get_cond_probas(mails:pd.DataFrame, labels:pd.DataFrame):\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    word_count_matrix = cv.fit_transform(mails.to_numpy()).toarray()\n",
    "\n",
    "    alpha = 1\n",
    "    unique_word_count = len(cv.get_feature_names())\n",
    "    words = np.array(cv.get_feature_names())\n",
    "\n",
    "    #instead of looping through the whole dataset counting word appearcances and adding probabilities, make \n",
    "    #an array that counts each word's appearances over the whole dataset (also considers how often a word\n",
    "    # appears in a sentence and not only if or if it doean's appear as in the 1.attempt)\n",
    "    spam_word_counts = word_count_matrix[labels == 1].sum(axis=0)\n",
    "    not_spam_word_counts = word_count_matrix[labels == 0].sum(axis=0)\n",
    "\n",
    "    #Calculating the contiditional probalbilities by going  spam_word_counts/ total amount of spam word\n",
    "    # + it adds laplace smoothing as seen here:  https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "    word_proba_spam = (spam_word_counts + alpha) / (spam_word_counts.sum() + alpha * unique_word_count)\n",
    "    word_proba_not_spam = (not_spam_word_counts + alpha) / (not_spam_word_counts.sum() + alpha * unique_word_count)\n",
    "\n",
    "    probabilities_df = pd.DataFrame({\n",
    "        'Word': words,\n",
    "        'P(Word|Spam)': word_proba_spam,\n",
    "        'P(Word|NotSpam)': word_proba_not_spam\n",
    "    })\n",
    "\n",
    "    return probabilities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>P(Word|Spam)</th>\n",
       "      <th>P(Word|NotSpam)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaenerfax</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aadedeji</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aagraw</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  P(Word|Spam)  P(Word|NotSpam)\n",
       "0          aa      0.000015         0.000124\n",
       "1         aaa      0.000005         0.000024\n",
       "2  aaaenerfax      0.000005         0.000003\n",
       "3    aadedeji      0.000005         0.000003\n",
       "4      aagraw      0.000005         0.000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GIve it a test\n",
    "data_path = \"../Data/Preprocessing/data_cooked.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "# somehow there are missing values somewhere, may have to check Preprocessing.ipynb again\n",
    "# very important to reset index! Because pandas would otherwise memorize the old indices which is bad for what is about to come\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "mails = dataframe['Message']\n",
    "labels = dataframe['Category']\n",
    "\n",
    "cond_probas = get_cond_probas(mails=mails,labels=labels)\n",
    "\n",
    "cond_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word  P(Word|Spam)  P(Word|NotSpam)\n",
      "3417       call      0.002998         0.002650\n",
      "4867    confirm      0.000252         0.000737\n",
      "9150       free      0.004293         0.000895\n",
      "9216     friend      0.000524         0.000376\n",
      "15768     money      0.003429         0.000252\n",
      "22408    signup      0.000098         0.000003\n",
      "24994  tomorrow      0.000113         0.000531\n",
      "26319     verif      0.000118         0.000019\n",
      "27228       win      0.000776         0.000094\n"
     ]
    }
   ],
   "source": [
    "#Just out of curiosity some smaple conditional probabilities\n",
    "examples = ['free', 'call', 'friend', \"tomorrow\", 'signup', 'money', 'win', 'verif', 'confirm']\n",
    "\n",
    "selected_probabilities = probabilities_df[probabilities_df['Word'].isin(examples)]\n",
    "\n",
    "print(selected_probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on slide number 45: Classifying spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for classifying a single message\n",
    "\n",
    "def classify_message(message, cond_probas, prior_probas):\n",
    "    \n",
    "\n",
    "    #the next line make a countvectorizer that uses the same \n",
    "    #columns names as in the training (column names = unique words)\n",
    "    #this is done by adding the same vocabulary as in Word\n",
    "    cv = CountVectorizer(vocabulary=cond_probas['Word'].values)\n",
    "\n",
    "    #with this CountVectorizer you can make a vector from the message\n",
    "    #same as the word_count_matrix, but only for one mail text\n",
    "    message_vector = cv.transform([message]).toarray().flatten()\n",
    "\n",
    "    #for the calculation it's somehow better to use log of the actual probabs\n",
    "    #avoids some kind of problem that arieses if one calculates with very small numbers\n",
    "    log_prob_spam = np.log(prior_probas['P_spam'])\n",
    "    log_prob_not_spam = np.log(prior_probas['P_not_spam'])\n",
    "\n",
    "    #in this loop the calculation for the vector is done for spam=yes as well as for spam=no\n",
    "    for word, count in zip(cv.get_feature_names(), message_vector): \n",
    "        if count > 0:  \n",
    "            if word in cond_probas['Word'].values: #could be that test data contains words that haavent been in training data\n",
    "                word_data = cond_probas[cond_probas['Word'] == word] # get the word's conditional probabilities\n",
    "                log_prob_spam += count * np.log(word_data['P(Word|Spam)'].values[0]) \n",
    "                log_prob_not_spam += count * np.log(word_data['P(Word|NotSpam)'].values[0])\n",
    "\n",
    "    #make the choice by comparing the two values\n",
    "    if log_prob_spam > log_prob_not_spam:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning\n",
    "# This cell may take a few minutes: \n",
    "# it loads the data, trains the model, makes predictions and calculates a score of the predictions\n",
    "\n",
    "\n",
    "#using train_test_split to make a train and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"../Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#mails = dataframe['Message']\n",
    "#labels = dataframe['Category']\n",
    "\n",
    "\n",
    "#making a train set which is used to calculate the probabilities and a test set to test afterwards with non seen cases\n",
    "mails_train, mails_test, labels_train, labels_test = train_test_split(\n",
    "    mails, labels, test_size=0.25, random_state=42,stratify=labels) #random_state shuffles all lines in the dataset, stratify balances the calsses spam and no spam because there's a lot more spam=no than spam=yes in our data\n",
    "\n",
    "\n",
    "#calling our functions from earlier\n",
    "prior_probas = get_prior_probas(mails_train, labels_train)\n",
    "cond_probas = get_cond_probas(mails_train, labels_train)\n",
    "\n",
    "#creating the result array with a list comprehension for each test mail in mails_test\n",
    "results = [classify_message(msg, cond_probas, prior_probas) for msg in mails_test]\n",
    "\n",
    "\n",
    "#calculate mean of an array that looks like this [True,False,False,True,True,True...]\n",
    "#using a list comprehension that zips together the model's predictions in results and the actual\n",
    "#labels in labels_test\n",
    "#then with pred == real it creates a True/False value, indicating, if the prediction was right/wrong\n",
    "#finally using np.mean to create the average\n",
    "accuracy = np.mean([pred == real for pred, real in zip(results, labels_test)])\n",
    "print(f\"Accuracy is: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another NBC example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9172</td>\n",
       "      <td>8592</td>\n",
       "      <td>sorri call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2115</td>\n",
       "      <td>1942</td>\n",
       "      <td>privat account statement show un redeem point ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Message                                                            \\\n",
       "           count unique                                                top   \n",
       "Category                                                                     \n",
       "0           9172   8592                                   sorri call later   \n",
       "1           2115   1942  privat account statement show un redeem point ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "Category       \n",
       "0          30  \n",
       "1           9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "data_path = \"../Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "\n",
    "dataframe.groupby('Category').describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8475x24338 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 381030 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train/test split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataframe.Message, dataframe.Category)\n",
    "\n",
    "# There were some null values in the Message column so I imputed the null values\n",
    "# with a default empty string \n",
    "dataframe['Message'].fillna(\"\", inplace=True)\n",
    "\n",
    "# get word count and store the data as a matrix \n",
    "cv = CountVectorizer()\n",
    "x_train_count = cv.fit_transform(x_train.values)\n",
    "\n",
    "x_train_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "naiveBayes = MultinomialNB()\n",
    "naiveBayes.fit(x_train_count, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-test for ham\n",
    "email_ham = [\"hey wanna meet up for dinner tonight?\"]\n",
    "email_ham_count = cv.transform(email_ham)\n",
    "naiveBayes.predict(email_ham_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-test for spam\n",
    "email_spam = [\"reward money insurance click win\"]\n",
    "email_spam_count = cv.transform(email_spam)\n",
    "naiveBayes.predict(email_spam_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415929203539823"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "x_test_count = cv.transform(x_test)\n",
    "naiveBayes.score(x_test_count, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model with tensorflow ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: to use tensorflow, you'll have to\n",
    "\n",
    "1. conda remove --name spam_classifier_env --all (to remove the old environment)\n",
    "2. conda env create -f environment.yml (to create the new enviroment containing keras)\n",
    "3. conda activate spam_classifier_env (to activate the new environment)\n",
    "4. restart notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "#importing the packages and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28166\n"
     ]
    }
   ],
   "source": [
    "#Load data and prepare\n",
    "data_path = \"../Data/Preprocessing/data_cooked.csv\"\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataframe = dataframe.dropna(subset=['Message']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = dataframe['Message']\n",
    "y = dataframe['Category']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(mails).toarray()  # make it a matrix, the network can handle (not sparse matrix)\n",
    "\n",
    "\n",
    "#this will tell how the input shape of the network needs to look like\n",
    "print(len(cv.get_feature_names()))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               14421504  \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14489857 (55.27 MB)\n",
      "Trainable params: 14488577 (55.27 MB)\n",
      "Non-trainable params: 1280 (5.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, input_shape=[X_train.shape[1]]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "283/283 [==============================] - 50s 174ms/step - loss: 0.0206 - accuracy: 0.9940 - val_loss: 0.1536 - val_accuracy: 0.9624\n",
      "Epoch 2/2\n",
      "283/283 [==============================] - 47s 166ms/step - loss: 0.0207 - accuracy: 0.9940 - val_loss: 0.1438 - val_accuracy: 0.9686\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1438 - accuracy: 0.9686\n",
      "Test Accuracy: 96.86%\n"
     ]
    }
   ],
   "source": [
    "#Takes a while, reduce epochs do less training...\n",
    "\n",
    "#10 epochs got me 97% accuracy and\n",
    "#2 epochs got me 96.9 accuracy\n",
    "# there's no big difference right now. Only if we did more finetuning, there would be more improvement\n",
    "#over the course of 10 epochs\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=2, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy on test data: {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam_classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
