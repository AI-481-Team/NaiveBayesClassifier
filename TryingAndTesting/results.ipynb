{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "this notebook uses code from the playground notebook as well as from the Models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make imports work\n",
    "import os\n",
    "os.chdir('/Users/csuftitan/Desktop/NaiveBayesClassifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import self_built_model\n",
    "from Models import deep_learning_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"model_NB_1\" : self_built_model.SelfBuiltNB_V1(),\n",
    "    \"model_NB_2\" : self_built_model.SelfBuiltNB_V2(),\n",
    "    \"model_MultiNB_sklearn\" : MultinomialNB(),\n",
    "    \"model_ComplementNB_sklearn\" : ComplementNB(),\n",
    "    \"model_logisticRegression\" : LogisticRegression(),\n",
    "    \"model_DL\"   : deep_learning_model.KerasModel()\n",
    "    #add more models here\n",
    "}\n",
    "\n",
    "#see how this preset is used in evaluation_dict below\n",
    "def get_new_eval_preset():\n",
    "    return {\n",
    "        \"accuracy\": 0,\n",
    "        \"f1_score\": 0,\n",
    "        \"confusion_matrix\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0\n",
    "    }\n",
    "\n",
    "#holds the evaluation results for each model\n",
    "evaluation_dict = {\n",
    "    \"model_NB_1\" : get_new_eval_preset(),\n",
    "    \"model_NB_2\" : get_new_eval_preset(),\n",
    "    \"model_MultiNB_sklearn\" : get_new_eval_preset(),\n",
    "    \"model_ComplementNB_sklearn\" : get_new_eval_preset(),\n",
    "    \"model_logisticRegression\" : get_new_eval_preset(),\n",
    "    \"model_DL\"   : get_new_eval_preset(),\n",
    "\n",
    "    #add more models here\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Preprocessing/data_cooked.csv\")\n",
    "data = data.dropna(subset=['Message']).reset_index(drop=True) #without this I get an error for missing values in training data (wonder why)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Self Built Model 1\n",
    "\n",
    "*WARNING*: Takes about 7:30 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Message'], data['Category'], test_size=0.3, random_state=42)\n",
    "\n",
    "model = model_dict[\"model_NB_1\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_NB_1'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_NB_1'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_1'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_1'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_1'][\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Self Built Model 2\n",
    "\n",
    "*WARNING*: Takes about 4:30 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Message'], data['Category'], test_size=0.3, random_state=42)\n",
    "\n",
    "model = model_dict[\"model_NB_2\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_NB_2'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_NB_2'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_2'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_2'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_NB_2'][\"accuracy\"] = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MultinomialNB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Message'], data['Category'], test_size=0.3, random_state=42)\n",
    "\n",
    "#using a pipeline can connect multiple steps:\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),  # creates a matrix with word counts\n",
    "    model_dict[\"model_MultiNB_sklearn\"]  # the classifier from scikit\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_MultiNB_sklearn'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_MultiNB_sklearn'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_MultiNB_sklearn'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_MultiNB_sklearn'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_MultiNB_sklearn'][\"accuracy\"] = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating ComplementNB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Message'], data['Category'], test_size=0.3, random_state=42)\n",
    "\n",
    "#using a pipeline can connect multiple steps:\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),  # creates a matrix with word counts\n",
    "    model_dict[\"model_ComplementNB_sklearn\"]  # the classifier from scikit\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_ComplementNB_sklearn'] = {}\n",
    "evaluation_dict['model_ComplementNB_sklearn'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_ComplementNB_sklearn'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_ComplementNB_sklearn'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_ComplementNB_sklearn'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_ComplementNB_sklearn'][\"accuracy\"] = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Logistic Regression from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Message'], data['Category'], test_size=0.3, random_state=42)\n",
    "\n",
    "#using a pipeline can connect multiple steps:\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),  # creates a matrix with word counts\n",
    "    model_dict[\"model_logisticRegression\"]  # the classifier from scikit\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_logisticRegression'] = {}\n",
    "evaluation_dict['model_logisticRegression'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_logisticRegression'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_logisticRegression'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_logisticRegression'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_logisticRegression'][\"accuracy\"] = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Keras DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = data['Message'] # make it a matrix, the network can handle (not sparse matrix)\n",
    "y = data['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.3, random_state=42)\n",
    "\n",
    "model = model_dict[\"model_DL\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#test and evaluation\n",
    "evaluation_dict['model_DL'][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "evaluation_dict['model_DL'][\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "evaluation_dict['model_DL'][\"precision\"] = precision_score(y_test, y_pred)\n",
    "evaluation_dict['model_DL'][\"recall\"] = recall_score(y_test, y_pred)\n",
    "evaluation_dict['model_DL'][\"accuracy\"] = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_NB_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_NB_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_MultiNB_sklearn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_logisticRegression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_logst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dict[\"model_DL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(40, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    values = [evaluation_dict[model][metric] for model in evaluation_dict]\n",
    "    ax.bar(evaluation_dict.keys(), values, color=['blue', 'green', 'red', 'purple'])\n",
    "    ax.set_title(f'{metric.capitalize()} Comparison')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_ylim(0.5, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plotten der Confusion Matrices\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (model_name, data) in zip(axes, evaluation_dict.items()):\n",
    "    ConfusionMatrixDisplay(data['confusion_matrix'], display_labels=['Class 0', 'Class 1']).plot(cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam_classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19 (default, Mar 20 2024, 15:00:34) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8605ccf3cc27d56dd733fe40c85b00c433ceec12335a598a7fac0fe6657b0a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
